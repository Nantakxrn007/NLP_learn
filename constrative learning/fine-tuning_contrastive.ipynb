{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa48d63",
   "metadata": {},
   "source": [
    "This homework is about learning sentence representation and contrastive learning.\n",
    "\n",
    "From previous homework, we used to build token/sequence classification task and learn it through only supervised method. In real-world scenario, human annotation requires a lot of cost and effort to do. Some annotation tasks might require domain experts such as medical domain, legal domain, etc. However, there are some unsupervised methods which are no need any annotations.\n",
    "\n",
    "Contrastive learning is the popular one of unsupervised learning approach. It will learn the representation via similar and dissimilar examples.\n",
    "\n",
    "For this homework, we will focus on SimCSE framework which is one of contrastive learning techniques. For SimCSE, it will learn sentence embedding by comparing between different views of the same sentence.\n",
    "\n",
    "In this homework you will perform three main tasks.\n",
    "\n",
    "Train a sentiment classification model using a pretrained model. This model uses freeze weights. That is it treats the pretrained model as a fixed feature extractor.\n",
    "Train a sentiment classification model using a pretrained model. This model also performs weight updates on the base model's weights.\n",
    "Perform SimCSE and use the sentence embedding to perform linear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbffac6",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9064bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdde3df",
   "metadata": {},
   "source": [
    "The dataset we use for this homework is Wisesight-Sentiment (huggingface, github) dataset. It is a Thai social media dataset which are labeled as 4 classes e.g. positive, negative, neutral, and question. Furthermore, It contains both Thai, English, Emoji, and etc. That is why we choose the distilled version of multilingual BERT (mBERT) DistilledBERT paper to be a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86d9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "dataset = load_dataset('pythainlp/wisesight_sentiment')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # Or a Thai-specific tokenizer if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f93f5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 21628\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2404\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['texts', 'category'],\n",
       "        num_rows: 2671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf8784d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': Value(dtype='string', id=None),\n",
       " 'category': ClassLabel(names=['pos', 'neu', 'neg', 'q'], id=None)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ee857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21628/21628 [00:03<00:00, 6005.99 examples/s]\n",
      "Map: 100%|██████████| 2404/2404 [00:00<00:00, 6269.95 examples/s]\n",
      "Map: 100%|██████████| 2671/2671 [00:00<00:00, 6343.00 examples/s]\n",
      "Map: 100%|██████████| 21628/21628 [00:00<00:00, 297418.43 examples/s]\n",
      "Map: 100%|██████████| 2404/2404 [00:00<00:00, 176695.12 examples/s]\n",
      "Map: 100%|██████████| 2671/2671 [00:00<00:00, 173300.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['texts'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Change `category` key to `labels`\n",
    "encoded_dataset = encoded_dataset.map(lambda examples: {'labels': [label for label in examples['category']]}, batched=True)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d20b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['texts', 'category', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 21628\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val) for key, val in self.encodings[idx].items()\n",
    "            if key in ['input_ids', 'attention_mask']\n",
    "        }\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec327194",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(encoded_dataset['train'], encoded_dataset['train']['labels'])\n",
    "val_dataset = SentimentDataset(encoded_dataset['validation'], encoded_dataset['validation']['labels'])\n",
    "test_dataset = SentimentDataset(encoded_dataset['test'], encoded_dataset['test']['labels'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c07ae",
   "metadata": {},
   "source": [
    "Base Model class  \n",
    "BaseModel is a parent class for building other models e.g.  \n",
    "\n",
    "Pretrained LM with a linear classifier  \n",
    "Fine-tuned LM with a linear classifier  \n",
    "Contrastive learning based (SimCSE) LM with a linear classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(LightningModule):\n",
    "    def __init__(self,model_name: str = 'distilbert-base-multilingual-cased', learning_rate: float = 2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        #output [batch, seq_len, hidden_size]\n",
    "        \n",
    "    def get_embeddings(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state\n",
    "        return cls_embedding[:,0,:]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.get_embeddings(input_ids, attention_mask)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4af7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier + freeze option\n",
    "class LMWithLinearClassfier(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilbert-base-multilingual-cased\",\n",
    "        ckpt_path: str = None,\n",
    "        learning_rate: float = 2e-5,\n",
    "        freeze_encoder_weights: bool = False,\n",
    "        num_classes: int = 4,\n",
    "    ):\n",
    "        super().__init__(model_name, learning_rate)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # TODO 2: load encoder's weights from checkpoint (ถ้ามี)\n",
    "        if ckpt_path:\n",
    "            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "        # TODO 3: linear classifier\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "        # freeze encoder weights (optional)\n",
    "        if freeze_encoder_weights:\n",
    "            self.freeze_weights(self.encoder)\n",
    "\n",
    "        # metric\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    # TODO 4: freeze encoder\n",
    "    def freeze_weights(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # TODO 5: forward pass (logits)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls_emb = super().forward(input_ids, attention_mask)\n",
    "        logits = self.classifier(cls_emb)\n",
    "        return logits\n",
    "\n",
    "    # TODO 6.1: training step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # TODO 6.2: validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    # TODO 6.3: test step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bfc2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eieiz\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "pretrained_lm_w_linear_model = LMWithLinearClassfier(\n",
    "    model_name,\n",
    "    ckpt_path=None,\n",
    "    freeze_encoder_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fea5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder    | DistilBertModel    | 134 M  | eval \n",
      "1 | classifier | Linear             | 3.1 K  | train\n",
      "2 | accuracy   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "134 M     Non-trainable params\n",
      "134 M     Total params\n",
      "538.949   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "92        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 676/676 [01:39<00:00,  6.78it/s, v_num=0, train_loss=1.050, train_acc=0.500, val_loss=1.050, val_acc=0.537]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 676: 'val_acc' reached 0.53702 (best 0.53702), saving model to 'D:\\\\NLP_learn\\\\NLP_learn\\\\constrative learning\\\\checkpoints\\\\best_pretrained_w_linear_model-epoch=0-val_acc=0.54.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 676/676 [01:40<00:00,  6.70it/s, v_num=0, train_loss=1.050, train_acc=0.429, val_loss=1.030, val_acc=0.537]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1352: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 676/676 [01:41<00:00,  6.68it/s, v_num=0, train_loss=0.860, train_acc=0.714, val_loss=1.030, val_acc=0.537]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2028: 'val_acc' reached 0.53744 (best 0.53744), saving model to 'D:\\\\NLP_learn\\\\NLP_learn\\\\constrative learning\\\\checkpoints\\\\best_pretrained_w_linear_model-epoch=2-val_acc=0.54.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 676/676 [01:43<00:00,  6.56it/s, v_num=0, train_loss=0.860, train_acc=0.714, val_loss=1.030, val_acc=0.537]\n"
     ]
    }
   ],
   "source": [
    "# Create a ModelCheckpoint callback (recommended way):\n",
    "pretrained_lm_w_linear_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_acc\",  # Metric to monitor\n",
    "    mode=\"max\",  # \"min\" for loss, \"max\" for accuracy\n",
    "    save_top_k=1,  # Save only the best model(s)\n",
    "    save_weights_only=True, # Saves only weights, not the entire model\n",
    "    dirpath=\"./checkpoints/\", # Path where the checkpoints will be saved\n",
    "    filename=\"best_pretrained_w_linear_model-{epoch}-{val_acc:.2f}\", # Customized name for the checkpoint\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "pretrained_lm_w_linear_trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='auto',\n",
    "    callbacks=[pretrained_lm_w_linear_checkpoint_callback], # Add the ModelCheckpoint callback\n",
    "    gradient_clip_val=1.0,\n",
    "    precision=16, # Mixed precision training\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "pretrained_lm_w_linear_trainer.fit(pretrained_lm_w_linear_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feeea592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\mini\\envs\\pine\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 84/84 [00:09<00:00,  8.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5439910292625427     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0267904996871948     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5439910292625427    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0267904996871948    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.0267904996871948, 'test_acc': 0.5439910292625427}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_lm_w_linear_result = pretrained_lm_w_linear_trainer.test(pretrained_lm_w_linear_model, test_loader)\n",
    "pretrained_lm_w_linear_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a98f6b",
   "metadata": {},
   "source": [
    "ลอง step fine-turning ตั้งแต่เริ่ม แต่คอมไม่ไหว"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
